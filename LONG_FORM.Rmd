---
title: "Long Form Narration"
author: "Natalie Ghidali"
date: "6/4/2021"
output: 
  html_document:
    number_sections: true
    code_folding: hide
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

We will be working with data on the amount of aid given through the Tuition Assistance Program (TAP) for New York universities from the years 2000-2019. The TAP is New York's largest financial aid grant program, assisting eligible New York residents pursue higher education. The dataset includes the dollar amounts of aid given to each recipient, as well as information regarding each recipients' family income, age group, university sector (e.g., public or private), level of study, and more. The dataset was provided by the New York State Higher Education Services Corporation, and it was downloaded from a New York state government data catalog. The dataset has about 288,000 rows, but we intend on focusing only on the years 2014-2019, which will reduce its size.

The original dataset can be accessed through the following link: <https://data.ny.gov/Education/Tuition-Assistance-Program-TAP-Recipients-Dollars-/2t78-bs45>

The objective of this analysis is to predict the sector type an applicant is applying for using the available variables. Therefore, the outcome variable is called `sector_type` and has two distinct classes: Public or Private.

# Data Processing

```{r, message = FALSE}
# loading packages
library(tidyverse)
library(tidymodels)
library(skimr)
library(janitor)
library(DT)
library(xgboost)
```

The original dataset was processed in order to make the analysis possible. The variables `level`, `tap_level_of_study`, `sector_type`, `tap_sector_group`, `tap_financial_status`, `tap_award_schedule`, `tap_degree_or_non_degree`, `tap_schedule_letter` and `recipient_age_group` were transformed into factors. The variable `income` was transformed from nominal income ranges to a numerical approximation. Finally, the rows were filtered so that only data as recent as 2014 was included.

```{r, message = FALSE}
tap_data <- read_csv("data/tap_dat.csv") %>% 
  clean_names() %>%
  mutate(
    level = factor(level),
    tap_level_of_study = factor(tap_level_of_study),
    sector_type = factor(sector_type),
    tap_sector_group = factor(tap_sector_group),
    recipient_age_group = ordered(recipient_age_group, 
                                  levels = c("under age 22",  "age 22 - 25", "age 26 - 35",  "age 36 - 50",  "over age 50")),
    tap_financial_status = factor(tap_financial_status),
    tap_award_schedule = factor(tap_award_schedule),
    tap_degree_or_non_degree = factor(tap_degree_or_non_degree),
    tap_schedule_letter = factor(tap_schedule_letter),
    income = as.numeric(sub(",","",str_extract(income_by_1_000_range, "\\d+,*\\d*")))
  ) %>%
  select(-income_by_1_000_range, -income_by_5_000_range, -income_by_10_000_range) %>% 
  filter(academic_year >= 2014)
```

# Exploratory Analysis

## Skim

The resulting dataset contains 56,846 observations and 14 features. Of the 14 variables, 9 are factors and 5 are numeric. The variable `tap_recipient_ft_es`, which refers to the number of TAP recipients. There are no instances of missingness in the dataset.

```{r}
# skim full dataset
skim(tap_data)
```

## Outcome Variable

It seems that there is a moderate class imbalance with the outcome variable, with most cases belonging to the `Public` class. Stratified sampling and resampling will be used going forward to minimize the issues of this imbalance.

```{r}
tap_data %>% ggplot(aes(sector_type, fill = sector_type)) +
  geom_bar() +
  scale_fill_discrete(
    guide = FALSE
  ) +
  labs(
    title = "Institution Type: Private vs. Public"
  ) +
  theme_bw()
```

```{r}
skim(tap_data$sector_type)
```

## Thorough univariate investigation of response and predictor variables

### Academic Year

Academic year is right skewed. There are no significant outliers. The year 2014 is the most represented and every subsequent year has less data.

```{r}
ggplot(tap_data, aes(academic_year)) +
  geom_histogram(bins = 20) +
  theme_bw()
```

### Level

It appears that the variable level is a constant. There is only one class represented. For this reason, this variable will not improve any of the models and will be excluded from the analysis.

```{r}
ggplot(tap_data, aes(level)) +
  geom_bar() +
  theme_bw()
```

### TAP Level of Study

There is a severe class imbalance with an equal amount of students in 2yr and 4yr undergraduate programs, very few in 5yr programs, and an extremely rare amount in remedial courses (STAP = Supplemental Tuition Assistance Program).

```{r}
ggplot(tap_data, aes(tap_level_of_study)) +
  geom_bar() +
  theme_bw()
```

### TAP Sector Group

Tap sector group is imbalanced. Most students belong in the 3-SUNY SO, which are 3 year SUNY State Operated Institutions. The second most represented sector is 5-Independent, which are Independent Colleges.

```{r}
ggplot(tap_data, aes(tap_sector_group)) +
  geom_bar() +
  coord_flip() +
  theme_bw()
```

### Recipient Age Group

Recipient age group has a fairly normal distribution with a slight right skew. The median age range is 22 - 25 years.

```{r}
ggplot(tap_data, aes(recipient_age_group)) +
  geom_bar() +
  theme_bw()
```

### TAP Financial Status

TAP financial status is imbalanced, with more financially independent cases than financially dependent.

```{r}
ggplot(tap_data, aes(tap_financial_status)) +
  geom_bar() +
  theme_bw()
```

### TAP Award Schedule

TAP award schedule is imbalanced, there are far more dependently scheduled then independently scheduled or married with no dependents.

```{r}
ggplot(tap_data, aes(tap_award_schedule)) +
  geom_bar() +
  theme_bw()
```

### TAP Degree or NonDegree

Tap degree or non degree is extremely imbalanced, with far more counts of degree then non degree. This means that almost all programs in this dataset are degree granting.

```{r}
ggplot(tap_data, aes(tap_degree_or_non_degree)) +
  geom_bar() +
  theme_bw()
```

### Income

There are more students at the lower end of the income spectrum vs at the higher end. Incomes do not exceed \$80,000, which is the limit to qualify for the program

```{r}
ggplot(tap_data, aes(income)) +
  geom_histogram(bins = 20) +
  theme_bw()
```

### TAP Recipient Headcount

The TAP recipient headcount is heavily right skewed with a very long tail. This measure is equal to the number of recipients as measured by students receiving at least one term award during the academic year.

```{r, warning = FALSE}
ggplot(tap_data, aes(tap_recipient_headcount)) +
  geom_histogram(bins = 50) +
  xlim(c(0,50)) +
  theme_bw()
```

### TAP Recipient FTEs

The TAP recipient FTEs is heavily right skewed with a very long tail. This measure equals to the number of recipients as measured by academic year Full Time Equivalents. Therefore, it makes sense that the distribution would resemble that of the TAP Recipient Headcount

```{r, warning = FALSE, message = FALSE}
ggplot(tap_data, aes(tap_recipient_ft_es)) +
  geom_histogram() +
  xlim(0,50)  +
  theme_bw()
```

### TAP Recipient Dollars

The TAP recipient dollars is heavily right skewed with a very long tail. Most recipients received less than \$25,000.

```{r, warning = FALSE, message = FALSE}
ggplot(tap_data, aes(tap_recipient_dollars)) +
  geom_histogram() +
  xlim(0,200000)  +
  theme_bw()
```

## Relationships between response variables and predictor variables

We first wanted to see the distribution of the total amount of aid given to students attending private or public schools.

```{r}
# Relationship between sector type and amount of aid awarded
ggplot(tap_data, aes(x = sector_type, y = tap_recipient_dollars)) +
  geom_bar(stat = "identity") +
  labs(x = "Sector Type",
       y = "Total Aid Given (USD)",
       title = "Total TAP Aid Given by Sector Type") +
  theme_bw()
```

There was more total aid allotted to students attending public schools than private schools

```{r}
# Income Level vs Sector Type
ggplot(tap_data, aes(x = sector_type, y = income)) +
  geom_boxplot() +
  labs(x = "Sector Type",
       y = "Income in USD",
       title = "Average Income Level Among Students by Sector Type") +
  theme_bw()
```

Students attending public schools have slightly higher average income which is unexpected given that public students receive more aid overall. This raises question about whether more public school students are financially dependent. However, the average income is only slightly higher among public school students than private school students, so we don't know to what extent this difference is important.

```{r}
# Financial Status and income

ggplot(tap_data, aes(x = tap_financial_status, y = income)) +
  geom_boxplot() +
  labs(x = "Financial Status", y = "Income (USD)", title = "Average Income of TAP Recipients by Financial Status") +
  theme_minimal()
```

Dependents have an average higher income than students who are financially independent

```{r}
# Income vs TAP Money Given
ggplot(tap_data, aes(x = income, y = tap_recipient_dollars)) +
  geom_line() +
  labs(x = "Income (USD)", y = "Amount of TAP Aid Given (USD)", title = "Amount of Aid Given By Income") +
  theme_minimal()
```

The amount of aid given to students decreases as their level of income increases.

# Model Fitting & Evaluation

To begin we started with a simple boosted tree model. We used all variables in our recipe and tuned mtry, min_n, and learn_rate. In our recipe, we made sure to add a step_other to deal with uncommon factor levels, because in tap_schedule_letter, levels F and G have only 2 observations each. We used a step_dummy to take care of our categorical variables and added a normalization step at the end to handle skews in several of our predictors. We stratified our data on the outcome sector_type, used an 80/20 split, and used a 5-fold cross validation on the training data with three repeats.

```{r message=FALSE, warning=FALSE, include=FALSE}
# set seed
set.seed(3948)

# loading data
tap_dat <- read.csv("data/tap_dat.csv")

# cleaning data
tap_dat <- read_csv("data/tap_dat.csv") %>% 
  clean_names() %>%
  filter(academic_year >= 2014) %>%
  filter(tap_level_of_study != "STAP") %>% # only one observation of STAP so remove it
  mutate(
    level = factor(level),
    tap_level_of_study = factor(tap_level_of_study),
    sector_type = factor(sector_type),
    tap_sector_group = factor(tap_sector_group),
    recipient_age_group = ordered(recipient_age_group, 
                                  levels = c("under age 22",  "age 22 - 25", "age 26 - 35",  "age 36 - 50",  "over age 50"),
                                  labels = c("under_age_22",  "age_22_25", "age_26_35",  "age_36_50",  "over_age_50")),
    tap_financial_status = factor(tap_financial_status),
    tap_award_schedule = factor(tap_award_schedule),
    tap_degree_or_non_degree = factor(tap_degree_or_non_degree),
    tap_schedule_letter = factor(tap_schedule_letter),
    income = as.numeric(sub(",","",str_extract(income_by_1_000_range, "\\d+,*\\d*")))
  ) %>%
  select(-income_by_1_000_range, -income_by_5_000_range, -income_by_10_000_range) %>%
  select(-tap_sector_group)  # sector group is too similar to sector type

# splitting data
tap_split <- initial_split(tap_dat, prop = .80, strata = sector_type)
tap_train <- training(tap_split)
tap_test <- testing(tap_split)

# creating folds
tap_folds <- vfold_cv(tap_train, v = 5, repeats = 3, strata = sector_type)

# tap_schedule_letter has some very uncommon factor levels, we will need to bin them in our recipe
count(tap_dat,tap_schedule_letter)

# creating the recipe
tap_recipe <- recipe(sector_type ~ ., data = tap_train) %>% 
  # remove level since it is duplicating information already contained in TAP level of study
  step_rm(level) %>%
  # Collapse uncommon factor levels, experimentally, a higher threshold works better, but this is extra necessary for tap_schedule_letter, where levels F and G have only 2 observations each
  step_other(all_nominal_predictors(), threshold = 0.05) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_normalize(all_numeric_predictors()) 
```

We set the model to use impurity for the importance metric and ran it. This model achieved a perfect 100% accuracy, so we knew we had done something wrong. After running a variable importance plot on our model results, we realized that level was by far the most prominent component in the model. Investigating our codebook, we discovered that tap_sector_group was perfectly predicting sector type. Tap sector group contained information on the type of the institution which is the same as our outcome variable. We removed tap_sector_group and ran the boosted tree model again, this time the accuracy was only 0.624, much more realistic. We then set about improving the model.

```{r, eval = FALSE}
# Boosted Tree Model
bt_model <- boost_tree(
  mode = "classification",
  mtry = tune(),
  min_n = tune(),
  learn_rate = tune()
) %>%
  set_engine("xgboost", importance = "impurity")

bt_workflow <- workflow() %>%
  add_recipe(tap_recipe) %>%
  add_model(bt_model)

bt_params <- parameters(bt_workflow) %>%
  update(mtry = mtry(range = c(1, 15)))

bt_grid <- grid_regular(bt_params, levels = 9)

control <- control_resamples(verbose = TRUE)

bt_tuned <- bt_workflow %>%
  tune_grid(tap_folds, bt_grid, control)
```

```{r eval=FALSE}
# Pick optimal tuning params
bt_results <- bt_workflow %>%
  finalize_workflow(select_best(bt_tuned, metric = "accuracy")) %>%
  fit(tap_train)

# Predict test set
bt_predictions <- predict(bt_results, new_data = tap_test) %>%
  bind_cols(tap_test %>% select(sector_type)) %>%
  rename(predicted = .pred_class) %>%
  mutate(predicted = factor(predicted),
         sector_type = factor(sector_type))

accuracy(bt_predictions,sector_type, predicted)
# 0.626
```

To determine interactions, we created a correlation matrix on our prepped and baked recipe

```{r}
correlations_data <- tap_recipe %>% 
  prep() %>% 
  bake(new_data = NULL) %>%
  select(-sector_type)
datatable(cor(correlations_data))
```

Based on the correlation matrix, we decided to remove tap_recipient_ft_es and tap_recipient_dollars because they were both over 95% correlated with tap_recipient_headcount. We also added interactions between tap_recipient_headcount and tap_recipient_age_group, tap_recipient_headcount and tap_financial_status, income and tap_financial_statues, and income and tap_award_schedule.

```{r}
tap_recipe <- recipe(sector_type ~ ., data = tap_train) %>%
  # remove level since it is duplicating information already contained in TAP level of study
  step_rm(level, tap_recipient_ft_es) %>%
  # Collapse uncommon factor levels, experimentally, a higher threshold works better, but this is extra necessary for tap_schedule_letter, where levels F and G have only 2 observations each
  step_other(all_nominal_predictors(), threshold = 0.05) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_interact(
    terms = ~ income:starts_with("tap_financial_status") + tap_recipient_headcount:starts_with("tap_financial_status") + tap_recipient_headcount:starts_with("recipient_age_group") + income:starts_with("tap_award_schedule")
  ) %>%
  step_normalize(all_numeric_predictors()) 
```

We reran the boosted tree model with the new recipe, and evaluated the accuracy.

```{r eval=FALSE}
# Boosted Tree Model
bt_model <- boost_tree(
  mode = "classification",
  mtry = tune(),
  min_n = tune(),
  learn_rate = tune()
) %>%
  set_engine("xgboost", importance = "impurity")

bt_workflow <- workflow() %>%
  add_recipe(tap_recipe) %>%
  add_model(bt_model)

bt_params <- parameters(bt_workflow) %>%
  update(mtry = mtry(range = c(1, 15)))

bt_grid <- grid_regular(bt_params, levels = 9)

control <- control_resamples(verbose = TRUE)

bt_tuned <- bt_workflow %>%
  tune_grid(tap_folds, bt_grid, control)
```

```{r eval=FALSE}
# Pick optimal tuning params
bt_results <- bt_workflow %>%
  finalize_workflow(select_best(bt_tuned, metric = "accuracy")) %>%
  fit(tap_train)

# Predict test set
bt_predictions <- predict(bt_results, new_data = tap_test) %>%
  bind_cols(tap_test %>% select(sector_type)) %>%
  rename(predicted = .pred_class) %>%
  mutate(predicted = factor(predicted),
         sector_type = factor(sector_type))

accuracy(bt_predictions,sector_type, predicted)
# 0.621
```

Unfortunately, the accuracy dropped from 0.626 in the model without interactions, to 0.621 in the model with interactions. To see which interactions and variables to keep, we conducted a variable importance analysis.

```{r}
bt_results <- readRDS("bt_results.rds")
datatable(arrange(xgb.importance(model=pull_workflow_fit(bt_results)$fit), desc(Gain)))
```

The variable importance chart highlights that the most usable interaction was income against financial status, we can keep this interaction and discard the rest.

Rerunning the boosted tree model with just the interaction between income and financial status produced an accuracy of 0.626. We decided to move forward with this recipe and try to tune a few new models.

We tried a k nearest neighbors model where we tuned the number of neighbors, and achieved an accuracy of 0.509.

```{r eval=FALSE}
# Define model ----
knn_model <- nearest_neighbor(mode = "classification",
                             neighbors = tune()) %>% 
  set_engine("kknn")

knn_workflow <- workflow() %>%
  add_recipe(tap_recipe) %>%
  add_model(knn_model)

# set-up tuning grid ----
knn_params <- parameters(knn_workflow) %>% 
  update(
    neighbors = neighbors(range = c(1, 15))
  )

knn_grid <- grid_regular(knn_params, levels = 9)

control <- control_resamples(verbose = TRUE)

knn_tuned <- knn_workflow %>%
  tune_grid(tap_folds, knn_grid, control)
```

```{r eval=FALSE}
# Pick optimal tuning params
show_best(knn_tuned, metric = "accuracy")
knn_results <- knn_workflow %>%
  finalize_workflow(select_best(knn_tuned, metric = "accuracy")) %>%
  fit(tap_train)

# Predict test set
knn_predictions <- predict(knn_results, new_data = tap_test) %>%
  bind_cols(tap_test %>% select(sector_type)) %>%
  rename(predicted = .pred_class) %>%
  mutate(predicted = factor(predicted),
         sector_type = factor(sector_type))

accuracy(knn_predictions,sector_type, predicted)
# 0.509
```

We tried a support vector machine using a radial basis function and achieved an accuracy of 0.636.

```{r eval=FALSE}
# Define model ----
svm_rbf_model <- svm_rbf(mode = "classification",
                         cost = tune(),
                         rbf_sigma = tune()) %>%
  set_engine("kernlab")

svm_rbf_workflow <- workflow() %>%
  add_recipe(tap_recipe) %>%
  add_model(svm_rbf_model)

svm_rbf_grid <-
  grid_regular(parameters(svm_rbf_workflow), levels = 3)

control <- control_resamples(verbose = TRUE)

svm_rbf_tuned <- svm_rbf_workflow %>%
  tune_grid(tap_folds, svm_rbf_grid, control)
```

```{r eval=FALSE}
# Pick optimal tuning params
show_best(svm_rbf_tuned, metric = "accuracy")
svm_rbf_results <- svm_rbf_workflow %>%
  finalize_workflow(select_best(svm_rbf_tuned, metric = "accuracy")) %>%
  fit(tap_train)

# Predict test set
svm_rbf_predictions <- predict(svm_rbf_results, new_data = tap_test) %>%
  bind_cols(tap_test %>% select(sector_type)) %>%
  rename(predicted = .pred_class) %>%
  mutate(predicted = factor(predicted),
         sector_type = factor(sector_type))

accuracy(svm_rbf_predictions,sector_type, predicted)
# accuracy = 0.636
```

We also tried a random forest model where we tuned mtry and min_n, here we found the greatest accuracy of 0.674.

```{r, eval = FALSE}
# Define model ----
rf_model <- rand_forest(
  mode = "classification",
  mtry = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity")

rf_workflow <- workflow() %>%
  add_recipe(tap_recipe) %>%
  add_model(rf_model)

rf_params <- parameters(rf_workflow) %>%
  update(mtry = mtry(range = c(1, 15)))

rf_grid <- grid_regular(rf_params, levels = 9)

control <- control_resamples(verbose = TRUE)

rf_tuned <- rf_workflow %>%
  tune_grid(tap_folds, rf_grid, control)
```

```{r, eval = FALSE}
# Pick optimal tuning params
show_best(rf_tuned, metric = "accuracy")
rf_results <- rf_workflow %>%
  finalize_workflow(select_best(rf_tuned, metric = "accuracy")) %>%
  fit(tap_train)

# Predict test set
rf_predictions <- predict(rf_results, new_data = tap_test) %>%
  bind_cols(tap_test %>% select(sector_type)) %>%
  rename(predicted = .pred_class) %>%
  mutate(predicted = factor(predicted),
         sector_type = factor(sector_type))

accuracy(rf_predictions,sector_type, predicted)
# 0.674
```

# Conclusion

We concluded that the random forest model would provide the best results when it came to new data. After the initial work of factoring our data, we explored our various variables. We found dramatic skew in our outcome variable and many of our predictors. Some predictors were removed due to being highly correlated. One of our predictors, tap_sector_group, was removed due to it being perfectly correlated to our outcome variable, creating a 100% prediction accuracy. The accuracy dropped significantly but was much more accurate. Our recipe was adjusted with each model. Ultimately, the random forest model proved to be the most accurate.

Being able to predict whether a college is public or private based on the amount of money it receives form the New York City government could allow us to better understand the influences of financial on student populations. One surprising thing we noticed is that public schools have a higher average income among its students but receive much higher levels of aid. This is an opportunity for further exploration.
